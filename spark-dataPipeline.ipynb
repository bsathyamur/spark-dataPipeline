{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType,StructField,DateType,TimestampType,StringType,IntegerType,DecimalType\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql.functions import year, month, dayofmonth,to_timestamp,to_date,split,substring,col,when\n",
    "from pyspark.sql.functions import udf\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparkExecutionLogger:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Adding information related to logger\n",
    "        self.logger = logging.getLogger('dev')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.fileHandler = logging.FileHandler('spark_log.log')\n",
    "        self.fileHandler.setLevel(logging.INFO)\n",
    "        self.logger.addHandler(self.fileHandler)\n",
    "        self.formatter = logging.Formatter('%(asctime)s  %(name)s  %(levelname)s: %(message)s')\n",
    "        self.fileHandler.setFormatter(self.formatter)\n",
    "        \n",
    "    def logMessage(self,info):\n",
    "        self.logger.info(info)\n",
    "\n",
    "class pipelineStage0:\n",
    "    \n",
    "    def __init__(self,configFile):\n",
    "        self.configFile = configFile\n",
    "        self.log = sparkExecutionLogger()\n",
    "        \n",
    "    def getConfig(self):\n",
    "        \n",
    "        try:            \n",
    "            self.log.logMessage(\"Fetching configuration values from config file\")\n",
    "            \n",
    "            with open(self.configFile) as f:\n",
    "                azure_blob_conn_values = yaml.load(f, Loader=yaml.FullLoader) \n",
    "            \n",
    "            storage_acct_name = azure_blob_conn_values['storage_account_name'].strip()\n",
    "            storage_acct_access_key = azure_blob_conn_values['storage_account_access_key'].strip()\n",
    "            storage_cont_name = azure_blob_conn_values['storage_container_name'].strip()\n",
    "            blob_output_dir = azure_blob_conn_values['blob_output_dir'].strip()\n",
    "            blob_conn_str = azure_blob_conn_values['blob_conn_string'].strip() \n",
    "        \n",
    "            blob_base_path =  \"wasbs://\" + storage_cont_name + \"@\" + storage_acct_name + \".blob.core.windows.net/\"\n",
    "            \n",
    "            return storage_acct_name,storage_acct_access_key,\\\n",
    "                    storage_cont_name,blob_output_dir,blob_base_path,blob_conn_str\n",
    "    \n",
    "        except Exception as e:        \n",
    "            self.log.logMessage(\"Exception \" + str(e))\n",
    "            return False\n",
    "\n",
    "class pipelineStage1:\n",
    "    \n",
    "    def __init__(self,contName,blobConnStr):\n",
    "        self.contName = contName\n",
    "        self.blobConnStr = blobConnStr\n",
    "        self.log = sparkExecutionLogger()        \n",
    "    \n",
    "    def ls_files(self,client,path,recursive=False):\n",
    "        \n",
    "        if not path == '' and not path.endswith('/'):\n",
    "            path += '/'\n",
    "\n",
    "        blob_iter = client.list_blobs(name_starts_with=path)\n",
    "        \n",
    "        files = []\n",
    "        \n",
    "        for blob in blob_iter:\n",
    "            relative_path = os.path.relpath(blob.name, path)\n",
    "            if recursive or not '/' in relative_path:\n",
    "                files.append(relative_path)\n",
    "        return files\n",
    "    \n",
    "\n",
    "    def getFileList(self,fileType,fileFilter):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            self.log.logMessage(\"Fetching list of files from the blob container\")\n",
    "            \n",
    "            blob_service_client = BlobServiceClient.from_connection_string(self.blobConnStr)\n",
    "            client = blob_service_client.get_container_client(self.contName)\n",
    "\n",
    "            files = self.ls_files(client, '', recursive=True)\n",
    "\n",
    "            file_list = []\n",
    "\n",
    "            for file in files:\n",
    "                if file[-4:] == fileType and fileFilter in file:\n",
    "                    file_list.append(str(file))        \n",
    "        \n",
    "            return file_list\n",
    "        \n",
    "        except Exception as e:        \n",
    "            self.log.logMessage(\"Exception \" + str(e))\n",
    "            return False    \n",
    "    \n",
    "class pipelineStage2:\n",
    "    \n",
    "    def __init__(self,sparkSession):\n",
    "        self.spark = sparkSession\n",
    "        self.log = sparkExecutionLogger()        \n",
    "        \n",
    "    def processData(self,filepath):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            self.log.logMessage(\"Processing file \" + filepath)\n",
    "            \n",
    "            df = self.spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(filepath)\n",
    "            df.na.fill('Guest',subset = [\"Customer ID\"])\n",
    "            df.na.fill('Unlisted',subset = [\"Description\"])\n",
    "            split_col = split(df['InvoiceDate'], '/')\n",
    "            df = df.withColumn(\"Year\",substring(split_col.getItem(2),1,4))\n",
    "            df = df.withColumn('month',split_col.getItem(0).cast(\"int\"))\n",
    "            df = df.withColumn('Qtr',(when( (col(\"month\") == 1) | (col(\"month\") == 2) | (col(\"month\") == 3),\"Qtr1\")\n",
    "                                        .when( (col(\"month\") == 4) | (col(\"month\") == 5) | (col(\"month\") == 6),\"Qtr2\")\n",
    "                                        .when( (col(\"month\") == 7) | (col(\"month\") == 8) | (col(\"month\") == 9) ,\"Qtr3\")\n",
    "                                        .otherwise(\"Qtr4\")))\n",
    "    \n",
    "            df = df.withColumn('InvoiceType',(when( (col(\"Quantity\") <= 0),\"Return\")\n",
    "                                                .otherwise(\"Purchase\")))        \n",
    "        \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.log.logMessage(\"Exception \" + str(e))\n",
    "            return False        \n",
    "    \n",
    "    def splitFiles(self,df,filepath):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            self.log.logMessage(\"Split files \" + filepath)\n",
    "            \n",
    "            df_uk = df.filter(df.Country == 'United Kingdom')\n",
    "            df_others = df.filter(df.Country != 'United Kingdom')        \n",
    "        \n",
    "            return df_uk,df_others \n",
    "        \n",
    "        except Exception as e:\n",
    "            self.log.logMessage(\"Exception \" + str(e))\n",
    "            return False          \n",
    "        \n",
    "    def writeFiles(self,outPath,df):\n",
    "        try:        \n",
    "            self.log.logMessage(\"Writing file to blob output directory \" + outPath)            \n",
    "            df.write.mode(\"append\").csv(outPath)\n",
    "            return True\n",
    "        \n",
    "        except Exception as e:            \n",
    "            self.log.logMessage(\"Exception \" + str(e))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    log = sparkExecutionLogger()\n",
    "    \n",
    "    try:\n",
    "        fileType = \".csv\"\n",
    "        fileFilter = 'input'\n",
    "        configFile = 'config.yml'\n",
    "        \n",
    "        # Getting all the config values from the config file\n",
    "        stg0 = pipelineStage0(configFile)\n",
    "        out0 = stg0.getConfig()\n",
    "    \n",
    "        if  out0 != False:    \n",
    "            acct_name,acct_key,cont_name,blob_output,blob_path,blob_conn = out0\n",
    "            \n",
    "            # Getting the list of new files in the input folder\n",
    "            stg1 = pipelineStage1(cont_name,blob_conn)\n",
    "            out1 = stg1.getFileList(fileType,fileFilter)\n",
    "    \n",
    "            if out1 != False:\n",
    "                fileList = out1\n",
    "                \n",
    "                # Creating a new spark session\n",
    "                spark = SparkSession.builder.appName(\"stockExchange\").getOrCreate()\n",
    "                key_str = \"fs.azure.account.key.\" + acct_name + \".blob.core.windows.net\"\n",
    "                spark.conf.set(key_str,acct_key)\n",
    "                \n",
    "                stg2 = pipelineStage2(spark)\n",
    "                outPath = blob_path + blob_output\n",
    "\n",
    "                for file in fileList:\n",
    "                    filepath =  blob_path + file \n",
    "                    \n",
    "                    # Processing the file data\n",
    "                    out2 = stg2.processData(filepath)\n",
    "            \n",
    "                    if out2 != False:\n",
    "                        df = out2\n",
    "                        \n",
    "                        #Splitting the files\n",
    "                        out3 = stg2.splitFiles(df,filepath)\n",
    "                \n",
    "                        if out3 != False:\n",
    "                            df_uk,df_others = out3\n",
    "                            \n",
    "                            # Writing the split files to the output blob\n",
    "                            if stg2.writeFiles(outPath,df_uk):\n",
    "                                stg2.writeFiles(outPath,df_others)\n",
    "    \n",
    "                spark.stop()    \n",
    "    except Exception as e:\n",
    "        log.logMessage(\"Exception \" + str(e))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
